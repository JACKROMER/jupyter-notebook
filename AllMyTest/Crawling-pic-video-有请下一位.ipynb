{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下载到哪里？(default is 'ooxx'): fiodasfjoasdjf\n",
      "下载多少页？(default is 10): 1\n",
      "D:\\GitHubProject\\jupyter-notebook\\AllMyTest\\downLoad\\blz18\\文件夹已存在\n",
      "文件存储文件夹： D:\\GitHubProject\\jupyter-notebook\\AllMyTest\\downLoad\\blz18\\fiodasfjoasdjf\n",
      "页面所有pageId ['/listhtml/1.html', '/listhtml/2.html', '/listhtml/3.html', '/listhtml/4.html', '/listhtml/5.html', '/listhtml/6.html', '/listhtml/7.html', '/listhtml/8.html', '/listhtml/9.html', '/listhtml/11.html', '/listhtml/12.html', '/listhtml/13.html', '/listhtml/14.html', '/listhtml/15.html', '/listhtml/16.html', '/listhtml/17.html', '/listhtml/18.html', '/listhtml/19.html', '/listhtml/1-3.html', '/listhtml/1.html', '/listhtml/1-2.html', '/listhtml/1-3.html', '/listhtml/1-5.html', '/listhtml/1-6.html', '/listhtml/1-7.html', '/listhtml/1-8.html', '/listhtml/1-9.html', '/listhtml/1-10.html', '/listhtml/1-11.html', '/listhtml/1-12.html', '/listhtml/1-13.html', '/listhtml/1-14.html', '/listhtml/1-15.html', '/listhtml/1-5.html', '/listhtml/1-175.html']\n",
      "当前页面地址 https://se.haodd148.com/listhtml/1.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "def url_open(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header('User-Agent','Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:36.0) Gecko/20100101 Firefox/36.0')\n",
    "    response = urllib.request.urlopen(req,timeout=120)\n",
    "    return response.read()\n",
    "\n",
    "# 获取page页码 1........9\n",
    "def get_page_id_arr(url):\n",
    "    html = url_open(url).decode('utf-8')\n",
    "#     print(html)\n",
    "    pattern = r'href=\"([^\"]*)\"[^>]*>.+?' # 匹配href\n",
    "    page_id_arr = re.findall(pattern, html)\n",
    "    page_id_arr_enable = []  # 过滤不需要的链接\n",
    "    for id in page_id_arr:\n",
    "        idstr = str(id)\n",
    "        if idstr.find('listhtml') != -1 : #筛选页码地址\n",
    "            page_id_arr_enable.append(idstr)\n",
    "    print(\"页面所有pageId\", page_id_arr_enable)\n",
    "    return page_id_arr_enable\n",
    "\n",
    "# 获取某页PAGEID中的所有相册链接地址\n",
    "def find_all_album_of_page(page_url):\n",
    "    print(\"当前页面地址\", page_url)\n",
    "    pattern = r'href=\"([^\"]*[0-9]{6}.html)\"[^>]*>.+?' # 匹配href\n",
    "    html = url_open(page_url).decode('utf-8')\n",
    "    alllinks = re.findall(pattern,html)\n",
    "    print(\"页面所有相册链接地址：\" , alllinks)\n",
    "    return alllinks\n",
    "\n",
    "# 进入某个相册获取所有图片链接\n",
    "def get_down_all_pic_of_album(folder, album_url):\n",
    "    pattern = r'src=\"([^\"]*.jpg)\"' # 匹配图片src地址\n",
    "    html = url_open(album_url).decode('utf-8')\n",
    "    print(html)\n",
    "    return\n",
    "    img_addrs = re.findall(pattern, html)\n",
    "    #https://img.yituyu.com/gallery/1070/00_qUYWQyHT.jpg\n",
    "    print(album_url + \"页面所有图片下载地址：\" , img_addrs)\n",
    "    # 下载此页面的所有图片\n",
    "    album_name = str(album_url)\n",
    "    save_imgs(folder,img_addrs, album_name) \n",
    "    print(\"暂停十秒获取下一个相册\")\n",
    "    time.sleep(10) #暂停一秒\n",
    "    \n",
    "# 保存图片到文件夹\n",
    "def save_imgs(folder, img_addrs, album_name):\n",
    "    sub_dir = album_name.replace(\"/\",\"\").replace(\":\",\"\").replace(\".\",\"\").replace(\"?\",\"\").replace(\"&\",\"\").replace(\"=\",\"\")\n",
    "    os.chdir(folder)\n",
    "    dirname = sub_dir\n",
    "    makeDir(dirname)\n",
    "    os.chdir(dirname)\n",
    "    for i in img_addrs:\n",
    "        pic_down_link = str(i)\n",
    "#         file_name_str = pic_down_link.split('/')\n",
    "#         filename = file_name_str[-1] # 文件名称 \n",
    "#         ranInt = ''.join(random.sample(string.ascii_letters + string.digits, 8))# 使用随机数\n",
    "        filename = pic_down_link.replace(\"/\",\"\").replace(\":\",\"\")\n",
    "        pic_pre = \"\" # 图片下载前缀\n",
    "        real_down_url = pic_pre + pic_down_link\n",
    "#       print(\"图片下载地址\", real_down_url)\n",
    "        image = url_open(real_down_url)\n",
    "#       print(\"文件名：\" + filename)\n",
    "#         with open(filename,'wb') as f:\n",
    "#             f.write(image)\n",
    "#             f.close()\n",
    "#             print(\"图片下载完成\", real_down_url)\n",
    "#             time.sleep(1) #暂停一秒\n",
    "\n",
    "# 创建文件夹\n",
    "def makeDir(folder):\n",
    "    if os.path.exists(folder) :\n",
    "        print(folder + \"文件夹已存在\")\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def download_some(folder='ooxx',pages=10):\n",
    "    current_dir = \"D:\\GitHubProject\\jupyter-notebook\\AllMyTest\\\\downLoad\\\\blz18\\\\\"\n",
    "    makeDir(current_dir)\n",
    "    folder = current_dir + folder\n",
    "    print(\"文件存储文件夹：\",folder)\n",
    "    makeDir(folder) #新建文件夹\n",
    "    os.chdir(folder) #跳转到文件夹\n",
    "    folder_top = os.getcwd() #获取当前工作目录\n",
    "    url = 'https://se.haodd148.com/listhtml/1-4.html'\n",
    "    page_id_arr = get_page_id_arr(url) #获取网页中的pageId\n",
    "    for page_id in page_id_arr:\n",
    "        page_url_pre = \"https://se.haodd148.com\" # page前缀\n",
    "        page_url = page_url_pre + page_id #组合page访问地址\n",
    "        all_album_link_arr = find_all_album_of_page(page_url) #获取PageId相册地址\n",
    "        print(\"暂停5秒获取下一个相册\")\n",
    "        time.sleep(5) #暂停一秒\n",
    "        # 进入页面找到播放地址\n",
    "        for album_link in all_album_link_arr:\n",
    "            # 图片服务器可能和访问服务器不是一个,需要组合\n",
    "            album_pre = \"https://se.haodd148.com\"\n",
    "            real_album_link = album_pre + album_link\n",
    "            # 进入页面下载此页面所有图片\n",
    "            get_down_all_pic_of_album(folder, real_album_link)\n",
    "    print(\"暂停十秒获取下一页\")\n",
    "    time.sleep(10) #暂停一秒\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder = input(\"下载到哪里？(default is 'ooxx'): \" )\n",
    "    pages  = input(\"下载多少页？(default is 10): \")\n",
    "    download_some(str(folder),int(pages))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tag/qiaotun/index_2.html']\n",
      "['https://xintu.crmvscrm.com/d/file/t/20201130/10/4ai2bpnkhlk.jpg']\n",
      "70cFvHSh_Q1YnxGkpoWK1HF6hhy\n",
      "201326592&amp\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "html = '<a href=\"/tag/qiaotun/index_2.html\">2</a>'\n",
    "pattern = r'href=\"([^\"]*)\"[^>]*>.+?' # 匹配href\n",
    "print(re.findall(pattern,html))\n",
    "pattern2 = r'src=\"([^\"]*)\"+?'\n",
    "html2 ='<a href=\"/xinggan/21610.html\" title=\"PartyCat轰趴猫 风骚翘臀少妇浴室性感蕾丝内衣摄影图片\" target=\"_blank\"><img src=\"https://xintu.crmvscrm.com/d/file/t/20201130/10/4ai2bpnkhlk.jpg\" data-original=\"https://xintu.crmvscrm.com/d/file/t/20201130/10/4ai2bpnkhlk.jpg\" alt=\"PartyCat轰趴猫 风骚翘臀少妇浴室性感蕾丝内衣摄影图片\" style=\"display: inline;\"></a>'\n",
    "print(re.findall(pattern2,html2))\n",
    "a = 'https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=3075027813,1435505077&amp;fm=26&amp;gp=0.jpg'\n",
    "b = a.split('/')\n",
    "print(b[3])\n",
    "mm = '/search/index?ct=201326592&amp;'\n",
    "print(mm[17:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/listhtml/1-3.html']\n",
      "['/html/177749.html']\n",
      "['https://newpic.haolepic.com/2020/nf1bh0zomdw505203.jpg']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "html ='<a href=\"/listhtml/1-3.html\">3</a>'\n",
    "pattern = r'href=\"([^\"]*)\"[^>]*>.+?' # 匹配href # 匹配href\n",
    "alllinks = re.findall(pattern,html)\n",
    "\n",
    "print(alllinks)\n",
    "html2 = '<a href=\"/html/177749.html\" target=\"_blank\">去野外让逼晒一晒</a>'\n",
    "pattern2 = r'href=\"([^\"]*[0-9]{2,10}.html)\"[^>]*>.+?' # 匹配href # 匹配href\n",
    "alllinks2 = re.findall(pattern2,html2)\n",
    "print(alllinks2)\n",
    "\n",
    "html3 ='<img src=\"https://newpic.haolepic.com/2020/nf1bh0zomdw505203.jpg\">'\n",
    "pattern3 = r'src=\"([^\"]*.jpg)\"' # 匹配href # 匹配href\n",
    "alllinks3 = re.findall(pattern3,html3)\n",
    "print(alllinks3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
